{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from langchain_core.documents import Document\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tiktoken\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "MAX_TOKENS = 400\n",
    "OVERLAP_SENTENCES = 1\n",
    "SIMILARITY_THRESHOLD = 0.75\n",
    "\n",
    "CLAUSE_RE = re.compile(\n",
    "    r\"(Section|Clause|Article)\\s+\\d+(\\.\\d+)*(\\([a-z]\\))*\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tok_len = lambda t: len(enc.encode(t))\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# HELPERS\n",
    "# -----------------------\n",
    "def extract_clauses(text):\n",
    "    return [\"\".join(m) for m in CLAUSE_RE.findall(text)]\n",
    "\n",
    "def find_offset(full, chunk, cursor):\n",
    "    pos = full.find(chunk, cursor)\n",
    "    return pos if pos != -1 else cursor\n",
    "\n",
    "# -----------------------\n",
    "# SEMANTIC CHUNKER\n",
    "# -----------------------\n",
    "def semantic_chunking(documents: List[Document],\n",
    "                      max_tokens: int = MAX_TOKENS,\n",
    "                      similarity_threshold: float = SIMILARITY_THRESHOLD\n",
    "                     ) -> List[Document]:\n",
    "\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "\n",
    "        text = doc.page_content\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        if len(sentences) < 3:\n",
    "            all_chunks.append(doc)\n",
    "            continue\n",
    "\n",
    "        # encode all sentences\n",
    "        embeddings = model.encode(sentences, normalize_embeddings=True)\n",
    "\n",
    "        # compute pairwise cosine similarity\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "        # group sentences by semantic similarity\n",
    "        # simple greedy grouping\n",
    "        visited = set()\n",
    "        groups = []\n",
    "\n",
    "        for i, sent in enumerate(sentences):\n",
    "            if i in visited:\n",
    "                continue\n",
    "\n",
    "            group = [i]\n",
    "            visited.add(i)\n",
    "\n",
    "            for j in range(i + 1, len(sentences)):\n",
    "                if j in visited:\n",
    "                    continue\n",
    "                if sim_matrix[i, j] >= similarity_threshold:\n",
    "                    group.append(j)\n",
    "                    visited.add(j)\n",
    "\n",
    "            groups.append(sorted(group))\n",
    "\n",
    "        # --- create token-bounded chunks ---\n",
    "        cursor = 0\n",
    "        chunk_idx = 0\n",
    "\n",
    "        for group in groups:\n",
    "            buf = []\n",
    "            buf_tokens = 0\n",
    "\n",
    "            for i in group:\n",
    "                sent = sentences[i]\n",
    "                t = tok_len(sent)\n",
    "\n",
    "                if buf_tokens + t > max_tokens and buf:\n",
    "                    # flush chunk\n",
    "                    chunk_text = \" \".join(buf)\n",
    "                    start = find_offset(text, chunk_text, cursor)\n",
    "                    end = start + len(chunk_text)\n",
    "\n",
    "                    meta = deepcopy(doc.metadata or {})\n",
    "                    meta.update({\n",
    "                        \"chunk_id\": f\"{doc_idx}_{chunk_idx}\",\n",
    "                        \"start_char\": start,\n",
    "                        \"end_char\": end,\n",
    "                        \"token_count\": tok_len(chunk_text),\n",
    "                        \"clauses\": extract_clauses(chunk_text)\n",
    "                    })\n",
    "\n",
    "                    all_chunks.append(Document(\n",
    "                        page_content=chunk_text,\n",
    "                        metadata=meta\n",
    "                    ))\n",
    "\n",
    "                    cursor = end\n",
    "                    chunk_idx += 1\n",
    "                    buf = buf[-OVERLAP_SENTENCES:]\n",
    "                    buf_tokens = tok_len(\" \".join(buf))\n",
    "\n",
    "                buf.append(sent)\n",
    "                buf_tokens += t\n",
    "\n",
    "            # flush remainder\n",
    "            if buf:\n",
    "                chunk_text = \" \".join(buf)\n",
    "                start = find_offset(text, chunk_text, cursor)\n",
    "                end = start + len(chunk_text)\n",
    "\n",
    "                meta = deepcopy(doc.metadata or {})\n",
    "                meta.update({\n",
    "                    \"chunk_id\": f\"{doc_idx}_{chunk_idx}\",\n",
    "                    \"start_char\": start,\n",
    "                    \"end_char\": end,\n",
    "                    \"token_count\": tok_len(chunk_text),\n",
    "                    \"clauses\": extract_clauses(chunk_text)\n",
    "                })\n",
    "\n",
    "                all_chunks.append(Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata=meta\n",
    "                ))\n",
    "\n",
    "                cursor = end\n",
    "                chunk_idx += 1\n",
    "\n",
    "    return all_chunks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
