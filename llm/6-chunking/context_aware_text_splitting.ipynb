{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79aec348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "from typing import List, Dict, Tuple\n",
    "from copy import deepcopy\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e94aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "\n",
    "MAX_TOKENS = 500\n",
    "OVERLAP_TOKENS = 80\n",
    "\n",
    "SECTION_PATTERNS = [\n",
    "    r\"^SECTION\\s+\\d+\",\n",
    "    r\"^ARTICLE\\s+\\d+\",\n",
    "    r\"^\\d+\\.\",\n",
    "    r\"^[A-Z][A-Z\\s]{5,}$\"\n",
    "]\n",
    "\n",
    "CLAUSE_RE = re.compile(\n",
    "    r\"(Section|Clause|Article)\\s+\\d+(\\.\\d+)*(\\([a-z]\\))*\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60fc2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "\n",
    "def tok_len(text):\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    return re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "\n",
    "def detect_sections(text):\n",
    "    sec_re = re.compile(\"|\".join(SECTION_PATTERNS))\n",
    "    current = \"INTRO\"\n",
    "    sections = []\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if sec_re.match(line.strip()):\n",
    "            current = line.strip()\n",
    "\n",
    "        sections.append((line, current))\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "def extract_clauses(text):\n",
    "    return [\"\".join(m) for m in CLAUSE_RE.findall(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662ceeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# CORE SPLITTER\n",
    "# -------------------------\n",
    "\n",
    "def context_aware_split(doc: Document) -> Tuple[List[Document], Dict]:\n",
    "\n",
    "    text = doc.page_content\n",
    "    parent_meta = doc.metadata or {}\n",
    "\n",
    "    sections = detect_sections(text)\n",
    "\n",
    "    # rebuild grouped by section\n",
    "    section_blocks: Dict[str, List[str]] = {}\n",
    "\n",
    "    for line, sec in sections:\n",
    "        section_blocks.setdefault(sec, []).append(line)\n",
    "\n",
    "    chunks = []\n",
    "    reverse_map = {}\n",
    "    cursor = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    for sec_title, lines in section_blocks.items():\n",
    "\n",
    "        section_text = \"\\n\".join(lines)\n",
    "        paragraphs = section_text.split(\"\\n\\n\")\n",
    "\n",
    "        for para in paragraphs:\n",
    "\n",
    "            sentences = split_sentences(para)\n",
    "\n",
    "            buf = []\n",
    "            buf_tokens = 0\n",
    "\n",
    "            for sent in sentences:\n",
    "\n",
    "                t = tok_len(sent)\n",
    "\n",
    "                if buf_tokens + t > MAX_TOKENS and buf:\n",
    "\n",
    "                    chunk_text = \" \".join(buf)\n",
    "                    start = text.find(chunk_text, cursor)\n",
    "                    if start == -1:\n",
    "                        start = cursor\n",
    "                    end = start + len(chunk_text)\n",
    "\n",
    "                    meta = deepcopy(parent_meta)\n",
    "                    meta.update({\n",
    "                        \"chunk_id\": f\"{chunk_idx}\",\n",
    "                        \"section_title\": sec_title,\n",
    "                        \"start_char\": start,\n",
    "                        \"end_char\": end,\n",
    "                        \"token_count\": tok_len(chunk_text),\n",
    "                        \"clauses\": extract_clauses(chunk_text)\n",
    "                    })\n",
    "\n",
    "                    chunks.append(Document(\n",
    "                        page_content=chunk_text,\n",
    "                        metadata=meta\n",
    "                    ))\n",
    "\n",
    "                    reverse_map[str(chunk_idx)] = meta\n",
    "                    chunk_idx += 1\n",
    "                    cursor = end\n",
    "\n",
    "                    # overlap window\n",
    "                    overlap_buf = buf[-2:]  # last sentences\n",
    "                    buf = overlap_buf.copy()\n",
    "                    buf_tokens = tok_len(\" \".join(buf))\n",
    "\n",
    "                buf.append(sent)\n",
    "                buf_tokens += t\n",
    "\n",
    "            # flush remainder\n",
    "            if buf:\n",
    "                chunk_text = \" \".join(buf)\n",
    "                start = text.find(chunk_text, cursor)\n",
    "                if start == -1:\n",
    "                    start = cursor\n",
    "                end = start + len(chunk_text)\n",
    "\n",
    "                meta = deepcopy(parent_meta)\n",
    "                meta.update({\n",
    "                    \"chunk_id\": f\"{chunk_idx}\",\n",
    "                    \"section_title\": sec_title,\n",
    "                    \"start_char\": start,\n",
    "                    \"end_char\": end,\n",
    "                    \"token_count\": tok_len(chunk_text),\n",
    "                    \"clauses\": extract_clauses(chunk_text)\n",
    "                })\n",
    "\n",
    "                chunks.append(Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata=meta\n",
    "                ))\n",
    "\n",
    "                reverse_map[str(chunk_idx)] = meta\n",
    "                chunk_idx += 1\n",
    "                cursor = end\n",
    "\n",
    "    stats = {\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"sections\": len(section_blocks)\n",
    "    }\n",
    "\n",
    "    return chunks, reverse_map, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80471b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks, reverse_map, stats = context_aware_split(document)\n",
    "# vector_db.add_documents(chunks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
